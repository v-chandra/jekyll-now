---
layout: page
title: Selected Recent Publications
permalink: /publications/
---

1. NASGEM: Neural architecture search via graph embedding method,
*AAAI 2021*. [[PDF]](https://arxiv.org/pdf/2007.04452.pdf)

1. ScaleNAS: One-Shot Learning of Scale-Aware Representations for
Visual Recognition,
*arXiv (2020)*. [[PDF]](https://arxiv.org/pdf/2011.14584.pdf)

1. AttentiveNAS: Improving Neural Architecture Search via Attentive Sampling,
*arXiv (2020)*. [[PDF]](https://arxiv.org/pdf/2011.09011.pdf)

1. Can Temporal Information Help with Contrastive Self-Supervised Learning?,
*arXiv (2020)*. [[PDF]](https://arxiv.org/pdf/2011.13046.pdf)

1. KeepAugment: A Simple Information-Preserving Data Augmentation Approach,
*arXiv (2020)*. [[PDF]](https://arxiv.org/pdf/2011.11778.pdf)

1. NASGEM: Neural architecture search via graph embedding method,
*AAAI 2021*. [[PDF]](https://arxiv.org/pdf/2007.04452.pdf)

1. DNA: Differentiable Network-Accelerator Co-Search,
*arXiv (2020)*. [[PDF]](https://arxiv.org/pdf/2010.14778.pdf)

1. One weight bitwidth to rule them all,
*Embedded Vision Workshop, ECCV 2020*. [[PDF]](https://arxiv.org/pdf/2008.09916.pdf)

1. Co-Exploration of Neural Architectures and Heterogeneous ASIC Accelerator 
Designs Targeting Multiple Tasks,
*DAC 2020*. [[PDF]](https://arxiv.org/pdf/2002.04116.pdf)

1. RecNMP: Accelerating Personalized Recommendation with Near-Memory Processing,
*ISCA 2020*. [[PDF]](https://arxiv.org/pdf/1912.12953.pdf)

1. Energy-Aware Neural Architecture Optimization With Splitting Steepest Descent, 
*Workshop on Energy Efficient Machine Learning and Cognitive Computing, NeurIPS (2019)*. [[PDF]](https://arxiv.org/pdf/1910.03103.pdf)

1. Improving Efficiency in Neural Network Accelerator using Operands Hamming Distance Optimization,
*Workshop on Energy Efficient Machine Learning and Cognitive Computing, NeurIPS (2019)*. [[PDF]](https://arxiv.org/pdf/2002.05293.pdf)

1. HERALD: Optimizing Heterogeneous DNN Accelerators for Edge Devices,
*arXiv (2019)*. [[PDF]](https://arxiv.org/pdf/1909.07437.pdf)

1. Federated Learning with Non-IID Data,
*arXiv (2018)*. [[PDF]](https://arxiv.org/pdf/1806.00582.pdf)

1. CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs,
*arXiv (2018)*. [[PDF]](https://arxiv.org/pdf/1801.06601.pdf)

1. Not All Ops are Created Equal!,
*arXiv (2018)*. [[PDF]](https://arxiv.org/pdf/1801.04326.pdf)

1. PrivyNet: A Flexible Framework for Privacy-Preserving Deep Neural Network Training,
*arXiv (2018)*. [[PDF]](https://arxiv.org/pdf/1709.06161.pdf)

1. Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Networks, 
*International Symposium on Computer Architecture, 2018*. [[PDF]](https://arxiv.org/pdf/1712.01507.pdf)

1. Hello Edge: Keyword Spotting on Microcontrollers, 
*arXiv (2017)*. [[PDF]](https://arxiv.org/pdf/1711.07128.pdf)

1. Deep Convolutional Neural Network Inference with Floating-point Weights and Fixed-point Activations,
*arXiv (2017)*. [[PDF]](https://arxiv.org/pdf/1703.03073.pdf)

1. Throughput-optimized OpenCL-based FPGA accelerator for large-scale convolutional neural networks,
*FPGA Conference (2016)*. [[PDF]](https://dl.acm.org/citation.cfm?id=2847276)
